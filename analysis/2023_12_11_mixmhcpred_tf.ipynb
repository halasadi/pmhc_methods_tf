{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc as calc_auc,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../public_data/mixmhcpred/TableS2.txt'\n",
    "# ignore the first row (header)\n",
    "df = pd.read_csv(path_to_data, sep='\\t', skiprows=1)\n",
    "X = df.drop('Allele', axis=1)\n",
    "y = list(df['Allele'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sequence.ljust(14, \"-\") for sequence in X['Peptide']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARNDCQEGHILKMFPSTWYV-\n",
      "[0, 0, 0, 8, 16, 8, 1, 18, 20, 20, 20, 20, 20, 20]\n",
      "AAAHTHRY------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "amino_acids = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "chars_peptide = amino_acids + [\"-\"]\n",
    "\n",
    "peptide_encoding_size = len(chars_peptide)\n",
    "print(''.join(chars_peptide))\n",
    "\n",
    "# create a mapping from amino-acids to integers\n",
    "stoi_peptide = { ch:i for i,ch in enumerate(chars_peptide) }\n",
    "itos_peptide = { i:ch for i,ch in enumerate(chars_peptide) }\n",
    "encode_peptide = lambda s: [stoi_peptide[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode_peptide = lambda l: ''.join([itos_peptide[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode_peptide(\"AAAHTHRY------\"))\n",
    "print(decode_peptide(encode_peptide(\"AAAHTHRY------\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([encode_peptide(sequence) for sequence in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now encode HLAs as integers\n",
    "hlas = list(set(y))\n",
    "hla_size = len(hlas)\n",
    "# create a mapping from amino-acids to integers\n",
    "stoi_hla = { ch:i for i,ch in enumerate(hlas) }\n",
    "itos_hla = { i:ch for i,ch in enumerate(hlas) }\n",
    "encode_hla = lambda s: [stoi_hla[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode_hla = lambda l: ''.join([itos_hla[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "y = np.array(encode_hla(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384070, 14]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# let's now encode the entire dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(X, dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "ydata = torch.tensor(y, dtype = torch.float)\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "train_y = ydata[:n].long()\n",
    "val_y = ydata[n:].long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "#import numpy as np\n",
    "\n",
    "\n",
    "# Define the Transformer-based model architecture.\n",
    "#class TransformerModel(nn.Module):\n",
    "#    def __init__(self, input_dim, output_dim, nhead, num_encoder_layers, hidden_dim, dropout):\n",
    "#        super(TransformerModel, self).__init__()\n",
    "#        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "#        self.transformer = nn.Transformer(\n",
    "#            d_model=hidden_dim,\n",
    "#            nhead=nhead,\n",
    "#            num_encoder_layers=num_encoder_layers,\n",
    "#            dim_feedforward=hidden_dim * 4,\n",
    "#            dropout=dropout,\n",
    "#            batch_first=True,\n",
    "#        )\n",
    "#        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#        \n",
    "#    def forward(self, x):\n",
    "#        x = self.embedding(x)\n",
    "#        x = self.transformer(x)\n",
    "#        x = x.mean(dim=1)  # Global average pooling\n",
    "#        x = self.fc(x)\n",
    "#        return x\n",
    "\n",
    "# Define hyperparameters and create the model instance.\n",
    "#input_dim = 20+1  # Example: dimension of one-hot encoding for amino acids\n",
    "#output_dim = 161  # Number of classes\n",
    "#nhead = 1  # Number of attention heads in the transformer\n",
    "#num_encoder_layers = 1  # Number of encoder layers in the transformer\n",
    "#hidden_dim = 9  # Hidden dimension of the transformer\n",
    "#dropout = 0.2\n",
    "\n",
    "#model = TransformerModel(input_dim, output_dim, nhead, num_encoder_layers, hidden_dim, dropout)\n",
    "#print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "max_iters = 1500\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 18\n",
    "n_head = 2\n",
    "n_layer = 2\n",
    "dropout = 0.25\n",
    "pmhc_length = 14\n",
    "# ------------\n",
    "\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ypred = train_y if split == 'train' else val_y\n",
    "   \n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    x = torch.stack([data[i] for i in ix])\n",
    "    y = torch.stack([ypred[i] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "       \n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple pMHC model\n",
    "class pMHCLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(peptide_encoding_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(pmhc_length, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, hla_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "       \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "       \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T)\n",
    "               \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            print(targets.shape)\n",
    "            print(logits.shape)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.063 K parameters\n",
      "torch.Size([32])\n",
      "torch.Size([32, 14, 119])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[448]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m==\u001b[39m max_iters \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39miter\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: train loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, val loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(split)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m out[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     \u001b[39mprint\u001b[39m(logits\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mview(B\u001b[39m*\u001b[39mT, C)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39;49mview(B\u001b[39m*\u001b[39;49mT)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, targets)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/halasadi/code/pmhc_methods_tf/analysis/2023_12_11_mixmhcpred_tf.ipynb#X12sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits, loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[448]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "model = pMHCLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "y_fake_data = torch.LongTensor([random.randint(0, 99) for _ in range(50)])  # Fake labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([34, 77, 74,  5, 38, 98, 80, 28, 25, 72, 83,  0, 29, 67, 74, 80, 44, 33,\n",
       "        65, 13,  0, 86, 68, 98, 49, 53, 22, 85, 16, 33, 82, 13, 21, 59,  4, 39,\n",
       "        93, 95, 98, 58, 93, 74, 91, 12, 27, 65, 66, 96, 62, 79])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
